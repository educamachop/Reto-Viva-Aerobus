{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUf5RSasOEqx",
        "outputId": "36582d94-88de-4a86-a3d9-e8826d6088af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns with NaNs: ['Aeronave']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv('Filghts TEC_Valid.csv')\n",
        "\n",
        "# Convert 'STD' to datetime\n",
        "df['STD'] = pd.to_datetime(df['STD'])\n",
        "\n",
        "# Filter data for the year 2023\n",
        "df = df[df['STD'].dt.year == 2023]\n",
        "\n",
        "# Initial drop of rows with NaN values in specific critical columns\n",
        "df.dropna(subset=['STD', 'DepartureStation', 'ArrivalStation', 'Passengers'], inplace=True)\n",
        "\n",
        "# Report if any NaNs remain in any other critical column\n",
        "if df.isnull().any().any():\n",
        "    print(\"Columns with NaNs:\", df.columns[df.isnull().any()].tolist())\n",
        "    df.dropna(inplace=True)  # Optionally remove all rows with any NaNs across the DataFrame\n",
        "\n",
        "# Check again if there are still NaNs after cleaning\n",
        "if df.isnull().any().any():\n",
        "    raise ValueError(\"NaNs remain after thorough cleaning and filtering.\")\n",
        "\n",
        "# Create necessary time-based features\n",
        "df['Month'] = df['STD'].dt.month\n",
        "df['Day'] = df['STD'].dt.day\n",
        "df['Weekday'] = df['STD'].dt.weekday\n",
        "df['Hour'] = df['STD'].dt.hour\n",
        "# Create a 'Route' feature combining 'DepartureStation' and 'ArrivalStation'\n",
        "\n",
        "# One-hot encode categorical variables 'Route' and 'Weekday'\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "categorical_features = encoder.fit_transform(df[['Weekday', 'DepartureStation', 'ArrivalStation']])\n",
        "categorical_feature_names = encoder.get_feature_names_out(['Weekday', 'DepartureStation', 'ArrivalStation'])\n",
        "\n",
        "# Convert encoded features into a DataFrame\n",
        "categorical_features_df = pd.DataFrame(categorical_features, columns=categorical_feature_names, index=df.index)\n",
        "\n",
        "# Concatenate with the original DataFrame\n",
        "df = pd.concat([df, categorical_features_df], axis=1)\n",
        "\n",
        "# Select only the features needed for modeling\n",
        "features = list(categorical_feature_names) + ['Month', 'Day', 'Hour', 'Capacity']\n",
        "X = df[features]\n",
        "y = df['Passengers']\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWck3iGVRngG",
        "outputId": "deb12f8a-edb1-486e-bf79-0f3c6fb75453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "7791/7791 [==============================] - 24s 3ms/step - loss: 1016.1520 - val_loss: 740.2439\n",
            "Epoch 2/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 721.7820 - val_loss: 706.7843\n",
            "Epoch 3/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 675.6935 - val_loss: 666.7831\n",
            "Epoch 4/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 635.3488 - val_loss: 632.9245\n",
            "Epoch 5/100\n",
            "7791/7791 [==============================] - 21s 3ms/step - loss: 614.2871 - val_loss: 618.7708\n",
            "Epoch 6/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 601.1207 - val_loss: 641.8728\n",
            "Epoch 7/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 588.9716 - val_loss: 626.9213\n",
            "Epoch 8/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 579.7370 - val_loss: 643.0914\n",
            "Epoch 9/100\n",
            "7791/7791 [==============================] - 21s 3ms/step - loss: 571.0580 - val_loss: 605.5718\n",
            "Epoch 10/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 565.1859 - val_loss: 589.6702\n",
            "Epoch 11/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 558.3723 - val_loss: 575.7413\n",
            "Epoch 12/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 551.7986 - val_loss: 586.3964\n",
            "Epoch 13/100\n",
            "7791/7791 [==============================] - 21s 3ms/step - loss: 544.8136 - val_loss: 573.3068\n",
            "Epoch 14/100\n",
            "7791/7791 [==============================] - 22s 3ms/step - loss: 539.8384 - val_loss: 590.9723\n",
            "Epoch 15/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 535.1004 - val_loss: 574.2214\n",
            "Epoch 16/100\n",
            "7791/7791 [==============================] - 22s 3ms/step - loss: 530.7334 - val_loss: 566.7933\n",
            "Epoch 17/100\n",
            "7791/7791 [==============================] - 18s 2ms/step - loss: 524.7856 - val_loss: 576.7318\n",
            "Epoch 18/100\n",
            "7791/7791 [==============================] - 21s 3ms/step - loss: 522.9988 - val_loss: 567.6906\n",
            "Epoch 19/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 518.3224 - val_loss: 563.3826\n",
            "Epoch 20/100\n",
            "7791/7791 [==============================] - 21s 3ms/step - loss: 516.5517 - val_loss: 564.1077\n",
            "Epoch 21/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 511.9376 - val_loss: 558.9796\n",
            "Epoch 22/100\n",
            "7791/7791 [==============================] - 21s 3ms/step - loss: 509.0402 - val_loss: 560.8564\n",
            "Epoch 23/100\n",
            "7791/7791 [==============================] - 23s 3ms/step - loss: 508.5764 - val_loss: 564.8939\n",
            "Epoch 24/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 504.4686 - val_loss: 569.8664\n",
            "Epoch 25/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 502.2494 - val_loss: 564.0021\n",
            "Epoch 26/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 499.6732 - val_loss: 554.1082\n",
            "Epoch 27/100\n",
            "7791/7791 [==============================] - 22s 3ms/step - loss: 498.5230 - val_loss: 560.8816\n",
            "Epoch 28/100\n",
            "7791/7791 [==============================] - 23s 3ms/step - loss: 496.7491 - val_loss: 553.0084\n",
            "Epoch 29/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 495.1068 - val_loss: 554.2157\n",
            "Epoch 30/100\n",
            "7791/7791 [==============================] - 22s 3ms/step - loss: 492.7094 - val_loss: 553.4855\n",
            "Epoch 31/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 491.1903 - val_loss: 547.8267\n",
            "Epoch 32/100\n",
            "7791/7791 [==============================] - 21s 3ms/step - loss: 489.8442 - val_loss: 561.4205\n",
            "Epoch 33/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 487.8688 - val_loss: 550.1865\n",
            "Epoch 34/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 486.3718 - val_loss: 551.5422\n",
            "Epoch 35/100\n",
            "7791/7791 [==============================] - 23s 3ms/step - loss: 485.7069 - val_loss: 549.8284\n",
            "Epoch 36/100\n",
            "7791/7791 [==============================] - 21s 3ms/step - loss: 484.2565 - val_loss: 547.1694\n",
            "Epoch 37/100\n",
            "7791/7791 [==============================] - 26s 3ms/step - loss: 482.4282 - val_loss: 550.5171\n",
            "Epoch 38/100\n",
            "7791/7791 [==============================] - 23s 3ms/step - loss: 480.4126 - val_loss: 551.6386\n",
            "Epoch 39/100\n",
            "7791/7791 [==============================] - 23s 3ms/step - loss: 479.8300 - val_loss: 543.8200\n",
            "Epoch 40/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 478.4657 - val_loss: 555.0623\n",
            "Epoch 41/100\n",
            "7791/7791 [==============================] - 22s 3ms/step - loss: 477.4850 - val_loss: 557.9963\n",
            "Epoch 42/100\n",
            "7791/7791 [==============================] - 24s 3ms/step - loss: 475.7377 - val_loss: 546.5445\n",
            "Epoch 43/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 473.8405 - val_loss: 556.0283\n",
            "Epoch 44/100\n",
            "7791/7791 [==============================] - 22s 3ms/step - loss: 473.1729 - val_loss: 549.0336\n",
            "Epoch 45/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 471.8109 - val_loss: 561.0422\n",
            "Epoch 46/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 470.4649 - val_loss: 543.8026\n",
            "Epoch 47/100\n",
            "7791/7791 [==============================] - 22s 3ms/step - loss: 469.4319 - val_loss: 547.4277\n",
            "Epoch 48/100\n",
            "7791/7791 [==============================] - 21s 3ms/step - loss: 469.3800 - val_loss: 542.6078\n",
            "Epoch 49/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 467.8377 - val_loss: 547.2379\n",
            "Epoch 50/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 466.5496 - val_loss: 541.2786\n",
            "Epoch 51/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 465.6325 - val_loss: 555.9083\n",
            "Epoch 52/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 464.8471 - val_loss: 555.2976\n",
            "Epoch 53/100\n",
            "7791/7791 [==============================] - 21s 3ms/step - loss: 464.9379 - val_loss: 540.4187\n",
            "Epoch 54/100\n",
            "7791/7791 [==============================] - 21s 3ms/step - loss: 462.6374 - val_loss: 537.1431\n",
            "Epoch 55/100\n",
            "7791/7791 [==============================] - 23s 3ms/step - loss: 462.9602 - val_loss: 568.5729\n",
            "Epoch 56/100\n",
            "7791/7791 [==============================] - 21s 3ms/step - loss: 462.0143 - val_loss: 546.1692\n",
            "Epoch 57/100\n",
            "7791/7791 [==============================] - 24s 3ms/step - loss: 460.7720 - val_loss: 539.1389\n",
            "Epoch 58/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 459.9682 - val_loss: 551.9639\n",
            "Epoch 59/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 458.5504 - val_loss: 546.6402\n",
            "Epoch 60/100\n",
            "7791/7791 [==============================] - 22s 3ms/step - loss: 458.2479 - val_loss: 546.8425\n",
            "Epoch 61/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 457.7769 - val_loss: 548.5630\n",
            "Epoch 62/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 456.4980 - val_loss: 539.8162\n",
            "Epoch 63/100\n",
            "7791/7791 [==============================] - 19s 2ms/step - loss: 454.8284 - val_loss: 542.4064\n",
            "Epoch 64/100\n",
            "7791/7791 [==============================] - 20s 3ms/step - loss: 454.2345 - val_loss: 548.4683\n",
            "Test MSE: 515.6856079101562\n",
            "761/761 [==============================] - 1s 2ms/step\n",
            "NN R^2 Score: 0.6336396213065971\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import r2_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# Assume 'X' and 'y' are already defined and split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scaling the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Model configuration\n",
        "input_dim = X_train_scaled.shape[1]  # Number of features\n",
        "output_dim = 1  # Regression output\n",
        "\n",
        "# Create a Sequential model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(output_dim))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Early stopping to prevent overfitting\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "model.fit(X_train_scaled, y_train, epochs=100, batch_size=10, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on test data\n",
        "loss = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f'Test MSE: {loss}')\n",
        "\n",
        "# Predictions and calculate R^2 Score\n",
        "predictions = model.predict(X_test_scaled)\n",
        "nn_r2 = r2_score(y_test, predictions)\n",
        "print(\"NN R^2 Score:\", nn_r2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('passengerReg.hdf5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzMvEygHOq2n",
        "outputId": "92bd0921-0998-4a0a-b485-634c2600d50f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load your data\n",
        "df2 = pd.read_csv('Filghts TEC_Valid.csv')\n",
        "\n",
        "# Convert 'STD' to datetime\n",
        "df2['STD'] = pd.to_datetime(df2['STD'])\n",
        "\n",
        "# Filter data for the year 2023\n",
        "df2 = df2[df2['STD'].dt.year == 2024]\n",
        "\n",
        "# Initial drop of rows with NaN values in specific critical columns\n",
        "df2.dropna(subset=['STD', 'DepartureStation', 'ArrivalStation', ], inplace=True)\n",
        "\n",
        "\n",
        "# Create necessary time-based features\n",
        "df2['Month'] = df2['STD'].dt.month\n",
        "df2['Day'] = df2['STD'].dt.day\n",
        "df2['Weekday'] = df2['STD'].dt.weekday\n",
        "df2['Hour'] = df2['STD'].dt.hour\n",
        "# Create a 'Route' feature combining 'DepartureStation' and 'ArrivalStation'\n",
        "\n",
        "# One-hot encode categorical variables 'Route' and 'Weekday'\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "categorical_features = encoder.fit_transform(df2[['Weekday', 'DepartureStation', 'ArrivalStation']])\n",
        "categorical_feature_names = encoder.get_feature_names_out(['Weekday', 'DepartureStation', 'ArrivalStation'])\n",
        "\n",
        "# Convert encoded features into a DataFrame\n",
        "categorical_features_df = pd.DataFrame(categorical_features, columns=categorical_feature_names, index=df2.index)\n",
        "\n",
        "# Concatenate with the original DataFrame\n",
        "df2 = pd.concat([df2, categorical_features_df], axis=1)\n",
        "\n",
        "# Select only the features needed for modeling\n",
        "features = list(categorical_feature_names) + ['Month', 'Day', 'Hour', 'Capacity']\n",
        "X = df2[features]\n",
        "y = df2['Passengers']\n",
        "X_scaled = scaler.transform(X)\n",
        "predictions = model.predict(X_scaled)\n",
        "df2['Passengers'] = predictions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPvc9qJrO9Sd",
        "outputId": "cc675393-fed7-49e1-f53e-d3a1a5f93554"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3807/3807 [==============================] - 6s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UwFdY2Agg-W",
        "outputId": "5b121799-eff3-4521-d0e7-d7ccd24d0a36"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Flight_ID', 'Aeronave', 'DepartureStation', 'ArrivalStation',\n",
              "       'Destination_Type', 'Origin_Type', 'STD', 'STA', 'Capacity',\n",
              "       'Passengers',\n",
              "       ...\n",
              "       'ArrivalStation_BJ', 'ArrivalStation_BK', 'ArrivalStation_BL',\n",
              "       'ArrivalStation_BM', 'ArrivalStation_BN', 'ArrivalStation_BO',\n",
              "       'ArrivalStation_BP', 'ArrivalStation_BQ', 'ArrivalStation_BS',\n",
              "       'ArrivalStation_BT'],\n",
              "      dtype='object', length=104)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = df2[['Flight_ID', 'Aeronave', 'DepartureStation', 'ArrivalStation',\n",
        "       'Destination_Type', 'Origin_Type', 'STD', 'STA', 'Capacity',\n",
        "       'Passengers']]"
      ],
      "metadata": {
        "id": "oqDAIuu4gX9T"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.to_csv('Preds2.csv')"
      ],
      "metadata": {
        "id": "YYDkWjEWgnNM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bfQRTfg_gsQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}